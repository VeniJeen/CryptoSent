{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814f32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "from tqdm import tqdm\n",
    "import text2emotion as te\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac03be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit=pd.read_csv(r\"C:\\Users\\Ben\\Desktop\\Diplomatiki\\Reddit 101\\Trying PRAW\\btc_bitcoin_2021_uptoAug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e7261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████▉                                          | 27260/62174 [01:32<01:59, 292.76it/s]"
     ]
    }
   ],
   "source": [
    "bit.selftext=bit.body.astype(str)\n",
    "bit.selftext=bit.body.fillna('NO TEXT')\n",
    "#bit=bit[bit.selftext!='nan']\n",
    "\n",
    "#english\n",
    "bit['lang']=bit.title.progress_apply(langid.classify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit['lang_text']=bit.body.progress_apply(langid.classify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit.lang=bit.lang.apply(lambda x: x[0])\n",
    "bit=bit[bit.lang=='en']\n",
    "\n",
    "bit.loc[bit.body=='nan','text']=bit.title\n",
    "bit.loc[bit.body!='nan','text']=bit.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c1365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bed83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d378c562",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da57b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "cached_stopwords=stopwords.words('english')\n",
    "\n",
    "remove_hashtags=lambda x: ' '.join(re.sub(\"(#[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "\n",
    "# removes hashtags mentions links and puncuation! \n",
    "regex_magic= lambda x: ' '.join(re.sub(\"(#[A-Za-z0-9]+)|(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "regex_notso_magic= lambda x: ' '.join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "#TEXT CLEANING FUNCTION\n",
    "def clean_text(text):\n",
    "    text=regex_magic(text).lower()\n",
    "    text= re.sub('[0-9]+', '', text)#removed numbers\n",
    "    tokens = re.split('\\W+', text)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in cached_stopwords]  # remove stopwords and stemming\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "#functions for printing the results of the Model\n",
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "sent140 = pd.read_csv(r\"D:\\DTU\\Semester 3 - Fall 2020\\Social Graphs and Interactions\\__Final Project__\\Data Proofs\\trainingandtestdata\\sentiment140.csv\",\n",
    "                      encoding=\"ISO-8859-1\" , names=[\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "#taking the columns that we need\n",
    "sent140 = sent140[['sentiment','text']]\n",
    "sent140['sentiment'] = sent140['sentiment'].replace(4,1)\n",
    "#preprocessing the text\n",
    "sent140['text_clean']=sent140.text.progress_apply(clean_text)\n",
    "sent140['text_clean_join']=sent140.text_clean.progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sent140['text_clean_join'],\n",
    "                                                    sent140.sentiment,\n",
    "                                                    test_size = 0.05, random_state = 42)\n",
    "word2vec = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\n",
    "word2vec.fit(X_train)\n",
    "X_train = word2vec.transform(X_train)\n",
    "X_test  = word2vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVC\n",
    "clf = LogisticRegression(C=2,n_jobs=-1,max_iter=10000)\n",
    "#Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "model_Evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a919e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the sentiment of the Tweets\n",
    "def get_sentiment(text):\n",
    "    text=word2vec.transform([text])\n",
    "    neg,pos=clf.predict_proba(text)[0]\n",
    "    if neg>.6:\n",
    "        sentiment='Negative'\n",
    "    elif pos>.6:\n",
    "        sentiment='Positive'\n",
    "    else:\n",
    "        sentiment='Netural'\n",
    "    return neg,pos,sentiment\n",
    "\n",
    "list2doc=lambda x: ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit['text_processed']=bit['text'].progress_apply(lambda x: list2doc(clean_text(x)))\n",
    "bit['sentiment_scores']=bit.text_processed.progress_apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit['Date']=pd.to_datetime(bit.date) \n",
    "bit=bit.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit.sample(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcp=pd.read_csv(r\"C:\\Users\\Ben\\Desktop\\Diplomatiki\\Datasets\\BTC_USD_2020-09-13_2021-09-12-CoinDesk.csv\")\n",
    "btcp['Date_index']=pd.to_datetime(btcp.Date)\n",
    "btcp=btcp.set_index('Date_index')\n",
    "\n",
    "btcp=btcp['2021':'2021-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9119ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=bit.resample('d').sum()[['polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d77358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([d1,btcp[['Closing Price (USD)']]],axis=1)\n",
    "df=df[df.polarity.notna()]\n",
    "df=df.rename(columns={'Closing Price (USD)':'btc_close'})\n",
    "df['btc_close_diff']=df.btc_close.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f53b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=df.resample('w').sum().btc_close_diff/60\n",
    "a2=df.resample('w').sum().polarity\n",
    "pd.concat([a1,a2],axis=1).plot(figsize=(15,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9db542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fff19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
