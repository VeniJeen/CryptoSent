{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#functions for printing the results of the Model\n",
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
    "\n",
    "#spacy clean text\n",
    "import spacy #load spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = stopwords.words(\"english\")\n",
    "regex_magic= lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "\n",
    "def text_preprocessing_spacy(comment, remove_stopwords):\n",
    "    comment = comment.lower()\n",
    "    comment=re.sub(r'http\\S+', '', comment)\n",
    "    comment=regex_magic(comment)\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    out=\" \".join(lemmatized)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "sent140 = pd.read_csv(r\"/home/s192851/CryptoSent_Heisenberg/Datasets/sentiment140.csv\",\n",
    "                      encoding=\"ISO-8859-1\" , names=[\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "#taking the columns that we need\n",
    "sent140 = sent140[['sentiment','text']]\n",
    "sent140['sentiment'] = sent140['sentiment'].replace(4,1)\n",
    "#preprocessing the text\n",
    "sent140['text_clean']=sent140.text.progress_apply(lambda x: text_preprocessing_spacy(x,True))\n",
    "#sent140['text_clean_join']=sent140.text_clean.progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.78     39999\n",
      "           1       0.77      0.80      0.79     40001\n",
      "\n",
      "    accuracy                           0.78     80000\n",
      "   macro avg       0.78      0.78      0.78     80000\n",
      "weighted avg       0.78      0.78      0.78     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sent140['text_clean'],\n",
    "                                                    sent140.sentiment,\n",
    "                                                    test_size = 0.05, random_state = 42)\n",
    "word2vec = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\n",
    "word2vec.fit(X_train)\n",
    "X_train = word2vec.transform(X_train)\n",
    "X_test  = word2vec.transform(X_test)\n",
    "\n",
    "#Linear SVC\n",
    "clf = LogisticRegression(C=2,n_jobs=-1,max_iter=10000)\n",
    "#Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "model_Evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the sentiment of the Tweets\n",
    "def get_sentiment(text):\n",
    "    text=word2vec.transform([text])\n",
    "    neg,pos=clf.predict_proba(text)[0]\n",
    "    if neg>.6:\n",
    "        sentiment='Negative'\n",
    "    elif pos>.6:\n",
    "        sentiment='Positive'\n",
    "    else:\n",
    "        sentiment='Netural'\n",
    "    return neg,pos,sentiment\n",
    "\n",
    "list2doc=lambda x: ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15061011/15061011 [49:00<00:00, 5122.46it/s]\n"
     ]
    }
   ],
   "source": [
    "bit['body_clean']=bit['body'].progress_apply(lambda x: text_preprocessing_spacy(x,True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit.to_pickle('../Datasets/comments.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bit=pd.read_pickle('../Datasets/comments.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15061011/15061011 [3:27:06<00:00, 1211.96it/s]\n"
     ]
    }
   ],
   "source": [
    "bit['sentiment_scores_lr']=bit.body_clean.progress_apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit.to_pickle('../Datasets/comments.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>award_name</th>\n",
       "      <th>award_description</th>\n",
       "      <th>award_count</th>\n",
       "      <th>award_coin_price</th>\n",
       "      <th>award_coin_reward</th>\n",
       "      <th>created</th>\n",
       "      <th>author_created</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>sentiment_scores_lr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:20</th>\n",
       "      <td>CommunistAndy</td>\n",
       "      <td>t2_1c4y3398</td>\n",
       "      <td>Crypto Expert | QC: ETH 22, BCH 20, BUTT 3</td>\n",
       "      <td>DOGE!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eczazky</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_ab4caa</td>\n",
       "      <td>t1_ecz56lx</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2wlj3</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2019-01-01 01:00:20</td>\n",
       "      <td>2018-05-09 15:25:59</td>\n",
       "      <td>doge</td>\n",
       "      <td>(0.3881488102866917, 0.6118511897133083, Posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:55</th>\n",
       "      <td>IGotThisYo</td>\n",
       "      <td>t2_f5ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You must have copied it from somewhere? You sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eczb12y</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_ab98uf</td>\n",
       "      <td>t1_ecz0sty</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2s3qj</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2019-01-01 01:00:55</td>\n",
       "      <td>2014-02-06 08:19:25</td>\n",
       "      <td>must copy somewhere know copy</td>\n",
       "      <td>(0.37687888336342323, 0.6231211166365768, Posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:01:41</th>\n",
       "      <td>ggori</td>\n",
       "      <td>t2_504hnn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;amp;#x200B;\\n\\nHappy new year boys &amp;lt;3!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eczb2xb</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_aa0clv</td>\n",
       "      <td>t3_aa0clv</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2s3qj</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2019-01-01 01:01:41</td>\n",
       "      <td>2017-06-16 20:40:43</td>\n",
       "      <td>amp x200b happy new year boy lt 3</td>\n",
       "      <td>(0.12090860555734628, 0.8790913944426537, Posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:02:05</th>\n",
       "      <td>antilex</td>\n",
       "      <td>t2_132607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.youtube.com/watch?v=xdJaDqm1RY4\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eczb3xw</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_ab6yu2</td>\n",
       "      <td>t1_ecz4mtz</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2s3qj</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2019-01-01 01:02:05</td>\n",
       "      <td>2016-11-25 06:07:06</td>\n",
       "      <td>dude piece chicken shit scum scour away tough ...</td>\n",
       "      <td>(0.26370068406783476, 0.7362993159321652, Posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:02:06</th>\n",
       "      <td>smellsliketuna</td>\n",
       "      <td>t2_4rqpy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;gt;Free speech doesn’t apply to what you say ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eczb3z4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_ab2olb</td>\n",
       "      <td>t1_eczao8v</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2si5v</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2019-01-01 01:02:06</td>\n",
       "      <td>2011-01-27 00:20:01</td>\n",
       "      <td>gt free speech apply say coworker customer emp...</td>\n",
       "      <td>(0.3277071673173454, 0.6722928326826546, Posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 01:59:54</th>\n",
       "      <td>kaykurokawa</td>\n",
       "      <td>t2_ap8ql</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If the politicians were smarter and younger, t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h3mfvqx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_oania1</td>\n",
       "      <td>t3_oania1</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2s3qj</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2021-07-01 01:59:54</td>\n",
       "      <td>2013-02-23 06:42:59</td>\n",
       "      <td>politician smart young actually try work way a...</td>\n",
       "      <td>(0.5153111155991015, 0.4846888844008985, Netural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 01:59:55</th>\n",
       "      <td>JohnnyTsunami1999</td>\n",
       "      <td>t2_3b0in4k</td>\n",
       "      <td>Platinum | QC: CC 94, ADA 65</td>\n",
       "      <td>It’s the easiest staking user experience I’m a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h3mfvti</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_ob6jz2</td>\n",
       "      <td>t3_ob6jz2</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2wlj3</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2021-07-01 01:59:55</td>\n",
       "      <td>2018-01-12 01:02:14</td>\n",
       "      <td>easy stake user experience aware forget though...</td>\n",
       "      <td>(0.4533763414602753, 0.5466236585397247, Netural)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 01:59:55</th>\n",
       "      <td>DDD420247</td>\n",
       "      <td>t2_4f8aqtur</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's not worth arguing over it, my pops is the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h3mfvu0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_oawc0n</td>\n",
       "      <td>t3_oawc0n</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2s3qj</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2021-07-01 01:59:55</td>\n",
       "      <td>2020-10-03 05:35:28</td>\n",
       "      <td>worth argue pop way stubborn heck go change mi...</td>\n",
       "      <td>(0.297838898932427, 0.702161101067573, Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 01:59:56</th>\n",
       "      <td>Wonzky</td>\n",
       "      <td>t2_15g8bh</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>High market cap, flexible staking, marketed a ton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h3mfvw6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_ob6jz2</td>\n",
       "      <td>t3_ob6jz2</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2wlj3</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2021-07-01 01:59:56</td>\n",
       "      <td>2017-02-16 07:53:20</td>\n",
       "      <td>high market cap flexible stake market ton</td>\n",
       "      <td>(0.11167811297384034, 0.8883218870261597, Posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 01:59:59</th>\n",
       "      <td>DamnItBrother</td>\n",
       "      <td>t2_7e2rfhxw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right most of then started as random accounts ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h3mfw2h</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_oasp6i</td>\n",
       "      <td>t3_oasp6i</td>\n",
       "      <td>...</td>\n",
       "      <td>t5_2wlj3</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>2021-07-01 01:59:59</td>\n",
       "      <td>2020-07-24 05:53:00</td>\n",
       "      <td>right much start random account go hello hello...</td>\n",
       "      <td>(0.11143075958155835, 0.8885692404184417, Posi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15061011 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                author author_fullname  \\\n",
       "created                                                  \n",
       "2019-01-01 01:00:20      CommunistAndy     t2_1c4y3398   \n",
       "2019-01-01 01:00:55         IGotThisYo        t2_f5ass   \n",
       "2019-01-01 01:01:41              ggori       t2_504hnn   \n",
       "2019-01-01 01:02:05            antilex       t2_132607   \n",
       "2019-01-01 01:02:06     smellsliketuna        t2_4rqpy   \n",
       "...                                ...             ...   \n",
       "2021-07-01 01:59:54        kaykurokawa        t2_ap8ql   \n",
       "2021-07-01 01:59:55  JohnnyTsunami1999      t2_3b0in4k   \n",
       "2021-07-01 01:59:55          DDD420247     t2_4f8aqtur   \n",
       "2021-07-01 01:59:56             Wonzky       t2_15g8bh   \n",
       "2021-07-01 01:59:59      DamnItBrother     t2_7e2rfhxw   \n",
       "\n",
       "                                              author_flair_text  \\\n",
       "created                                                           \n",
       "2019-01-01 01:00:20  Crypto Expert | QC: ETH 22, BCH 20, BUTT 3   \n",
       "2019-01-01 01:00:55                                         NaN   \n",
       "2019-01-01 01:01:41                                         NaN   \n",
       "2019-01-01 01:02:05                                         NaN   \n",
       "2019-01-01 01:02:06                                         NaN   \n",
       "...                                                         ...   \n",
       "2021-07-01 01:59:54                                         NaN   \n",
       "2021-07-01 01:59:55                Platinum | QC: CC 94, ADA 65   \n",
       "2021-07-01 01:59:55                                         NaN   \n",
       "2021-07-01 01:59:56                                      Bronze   \n",
       "2021-07-01 01:59:59                                         NaN   \n",
       "\n",
       "                                                                  body  \\\n",
       "created                                                                  \n",
       "2019-01-01 01:00:20                                             DOGE!    \n",
       "2019-01-01 01:00:55  You must have copied it from somewhere? You sh...   \n",
       "2019-01-01 01:01:41         &amp;#x200B;\\n\\nHappy new year boys &lt;3!   \n",
       "2019-01-01 01:02:05  https://www.youtube.com/watch?v=xdJaDqm1RY4\\n\\...   \n",
       "2019-01-01 01:02:06  &gt;Free speech doesn’t apply to what you say ...   \n",
       "...                                                                ...   \n",
       "2021-07-01 01:59:54  If the politicians were smarter and younger, t...   \n",
       "2021-07-01 01:59:55  It’s the easiest staking user experience I’m a...   \n",
       "2021-07-01 01:59:55  It's not worth arguing over it, my pops is the...   \n",
       "2021-07-01 01:59:56  High market cap, flexible staking, marketed a ton   \n",
       "2021-07-01 01:59:59  Right most of then started as random accounts ...   \n",
       "\n",
       "                    distinguished       id  is_submitter no_follow    link_id  \\\n",
       "created                                                                         \n",
       "2019-01-01 01:00:20           NaN  eczazky         False      True  t3_ab4caa   \n",
       "2019-01-01 01:00:55           NaN  eczb12y         False      True  t3_ab98uf   \n",
       "2019-01-01 01:01:41           NaN  eczb2xb         False      True  t3_aa0clv   \n",
       "2019-01-01 01:02:05           NaN  eczb3xw         False      True  t3_ab6yu2   \n",
       "2019-01-01 01:02:06           NaN  eczb3z4         False      True  t3_ab2olb   \n",
       "...                           ...      ...           ...       ...        ...   \n",
       "2021-07-01 01:59:54           NaN  h3mfvqx         False     False  t3_oania1   \n",
       "2021-07-01 01:59:55           NaN  h3mfvti         False     False  t3_ob6jz2   \n",
       "2021-07-01 01:59:55           NaN  h3mfvu0         False      True  t3_oawc0n   \n",
       "2021-07-01 01:59:56           NaN  h3mfvw6         False      True  t3_ob6jz2   \n",
       "2021-07-01 01:59:59           NaN  h3mfw2h         False      True  t3_oasp6i   \n",
       "\n",
       "                      parent_id  ... subreddit_id  award_name  \\\n",
       "created                          ...                            \n",
       "2019-01-01 01:00:20  t1_ecz56lx  ...     t5_2wlj3       Empty   \n",
       "2019-01-01 01:00:55  t1_ecz0sty  ...     t5_2s3qj       Empty   \n",
       "2019-01-01 01:01:41   t3_aa0clv  ...     t5_2s3qj       Empty   \n",
       "2019-01-01 01:02:05  t1_ecz4mtz  ...     t5_2s3qj       Empty   \n",
       "2019-01-01 01:02:06  t1_eczao8v  ...     t5_2si5v       Empty   \n",
       "...                         ...  ...          ...         ...   \n",
       "2021-07-01 01:59:54   t3_oania1  ...     t5_2s3qj       Empty   \n",
       "2021-07-01 01:59:55   t3_ob6jz2  ...     t5_2wlj3       Empty   \n",
       "2021-07-01 01:59:55   t3_oawc0n  ...     t5_2s3qj       Empty   \n",
       "2021-07-01 01:59:56   t3_ob6jz2  ...     t5_2wlj3       Empty   \n",
       "2021-07-01 01:59:59   t3_oasp6i  ...     t5_2wlj3       Empty   \n",
       "\n",
       "                    award_description award_count award_coin_price  \\\n",
       "created                                                              \n",
       "2019-01-01 01:00:20             Empty       Empty            Empty   \n",
       "2019-01-01 01:00:55             Empty       Empty            Empty   \n",
       "2019-01-01 01:01:41             Empty       Empty            Empty   \n",
       "2019-01-01 01:02:05             Empty       Empty            Empty   \n",
       "2019-01-01 01:02:06             Empty       Empty            Empty   \n",
       "...                               ...         ...              ...   \n",
       "2021-07-01 01:59:54             Empty       Empty            Empty   \n",
       "2021-07-01 01:59:55             Empty       Empty            Empty   \n",
       "2021-07-01 01:59:55             Empty       Empty            Empty   \n",
       "2021-07-01 01:59:56             Empty       Empty            Empty   \n",
       "2021-07-01 01:59:59             Empty       Empty            Empty   \n",
       "\n",
       "                    award_coin_reward             created      author_created  \\\n",
       "created                                                                         \n",
       "2019-01-01 01:00:20             Empty 2019-01-01 01:00:20 2018-05-09 15:25:59   \n",
       "2019-01-01 01:00:55             Empty 2019-01-01 01:00:55 2014-02-06 08:19:25   \n",
       "2019-01-01 01:01:41             Empty 2019-01-01 01:01:41 2017-06-16 20:40:43   \n",
       "2019-01-01 01:02:05             Empty 2019-01-01 01:02:05 2016-11-25 06:07:06   \n",
       "2019-01-01 01:02:06             Empty 2019-01-01 01:02:06 2011-01-27 00:20:01   \n",
       "...                               ...                 ...                 ...   \n",
       "2021-07-01 01:59:54             Empty 2021-07-01 01:59:54 2013-02-23 06:42:59   \n",
       "2021-07-01 01:59:55             Empty 2021-07-01 01:59:55 2018-01-12 01:02:14   \n",
       "2021-07-01 01:59:55             Empty 2021-07-01 01:59:55 2020-10-03 05:35:28   \n",
       "2021-07-01 01:59:56             Empty 2021-07-01 01:59:56 2017-02-16 07:53:20   \n",
       "2021-07-01 01:59:59             Empty 2021-07-01 01:59:59 2020-07-24 05:53:00   \n",
       "\n",
       "                                                            body_clean  \\\n",
       "created                                                                  \n",
       "2019-01-01 01:00:20                                               doge   \n",
       "2019-01-01 01:00:55                      must copy somewhere know copy   \n",
       "2019-01-01 01:01:41                  amp x200b happy new year boy lt 3   \n",
       "2019-01-01 01:02:05  dude piece chicken shit scum scour away tough ...   \n",
       "2019-01-01 01:02:06  gt free speech apply say coworker customer emp...   \n",
       "...                                                                ...   \n",
       "2021-07-01 01:59:54  politician smart young actually try work way a...   \n",
       "2021-07-01 01:59:55  easy stake user experience aware forget though...   \n",
       "2021-07-01 01:59:55  worth argue pop way stubborn heck go change mi...   \n",
       "2021-07-01 01:59:56          high market cap flexible stake market ton   \n",
       "2021-07-01 01:59:59  right much start random account go hello hello...   \n",
       "\n",
       "                                                   sentiment_scores_lr  \n",
       "created                                                                 \n",
       "2019-01-01 01:00:20  (0.3881488102866917, 0.6118511897133083, Posit...  \n",
       "2019-01-01 01:00:55  (0.37687888336342323, 0.6231211166365768, Posi...  \n",
       "2019-01-01 01:01:41  (0.12090860555734628, 0.8790913944426537, Posi...  \n",
       "2019-01-01 01:02:05  (0.26370068406783476, 0.7362993159321652, Posi...  \n",
       "2019-01-01 01:02:06  (0.3277071673173454, 0.6722928326826546, Posit...  \n",
       "...                                                                ...  \n",
       "2021-07-01 01:59:54  (0.5153111155991015, 0.4846888844008985, Netural)  \n",
       "2021-07-01 01:59:55  (0.4533763414602753, 0.5466236585397247, Netural)  \n",
       "2021-07-01 01:59:55   (0.297838898932427, 0.702161101067573, Positive)  \n",
       "2021-07-01 01:59:56  (0.11167811297384034, 0.8883218870261597, Posi...  \n",
       "2021-07-01 01:59:59  (0.11143075958155835, 0.8885692404184417, Posi...  \n",
       "\n",
       "[15061011 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('../Datasets/comments.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c1795bb0eb239664b2131c9b52e756eacf4bc94d3cc271283108f2c926c8d69"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sandbox': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
